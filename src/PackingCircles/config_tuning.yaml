# This is a configuration file for hyperparameter tuning.
# The file will be passed to instances of a series of Tuner() in which
# performance profile is calculated according to the setting in this configuration file.

# Tutorial for your project:
# 1. Edit 'project_name' to match your project's name.
# 2. Adjust 'problem_instances', 'problem_initialpoint', and 'problem_coordinator_name' to suit your experiments.
#    Ensure that 'problem_coordinator_name' corresponds to the filename of your custom ProblemCoordinator subclass.
# 3. Specify the 'solver_name' whose hyperparameters you want to tune.
# 4. Customize 'solver_evaluation', 'max_threshold', 'threshold_step', and 'save_experiment'.
#    'solver_evaluation' should represent the metric used to evaluate the solver's performance during tuning.
#    Common options include 'residual' in nonlinear optimization, 'time', or 'cost' in other scenarios.
# 5. Edit 'solver_option' to define the set of hyperparameters whose combinations will be examined during the tuning session.
#
# Next, proceed to the 'tuner.py' file.


problem_name: 'PackingCircles'
problem_instance: [1, 2] # [1,2,3,4,5,6,7,8,9,10]
problem_initialpoint: a
problem_coordinator_name: "coordinator"
solver_name: "RIPTRM"

# Setting for hyperparameter tuning
solver_evaluation: "residual"  # evaluation index used in profiling
max_threshold: 10  # max threshold for describing performance profile
threshold_step: 50  # the number of increments in x-axis of performance profile
save_experiment: True  # whether saving the experimental results by each solver setting
output_path: tuning/${problem_name}_${solver_name}

# Set of hyperparameters that will be tuned
solver_option:
  barrier_parameter_update_r: [0.01, 0.3, 0.5]
  barrier_parameter_update_c: [5, 10]
  barrier_parameter_update_b: [0.5, 0.8]
  # barrier_parameter_update_r: [0.3, 0.5]
  # barrier_parameter_update_c: [0.1, 10]
  # barrier_parameter_update_b: [0.3, 0.5, 0.8]

